{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (4.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: sacremoses in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: requests in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: joblib in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with HuggingFace - Text Generation\n",
    "\n",
    "\n",
    "\n",
    "**In this notebook, we will explore different decoding methods like Beam search, Top-K sampling, and Top-P sampling, demonstrating their performance along the way. This project is a work in progress and I will rigourslly update it as I learn more about text generation. Feel free to comment with any questions/suggestions:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproducability\n",
    "SEED = 70\n",
    "\n",
    "#maximum number of words in output text\n",
    "MAX_LEN = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Intro\n",
    "\n",
    "** GPT-2 is capable of next word prediction on a much larger and more gigiatuas scale.**\n",
    "\n",
    "**Transformers makes it very easy to import this model with both PyTorch and TensorFlow - in this notebook we will be using TensorFlow but it is just as easy in PyTorch. Both the model and its Tokenizer can be imported from the `transformers` library that anyone can get by typing `!pip install transformers`. We begin with our input seqquence:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = \"after such a tiring day at work, i come back and \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (22.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jax in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (0.3.13)\n",
      "Requirement already satisfied: jaxlib in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (0.3.10)\n",
      "Requirement already satisfied: typing-extensions in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jax) (3.7.4.3)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jax) (1.6.2)\n",
      "Requirement already satisfied: absl-py in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jax) (1.0.0)\n",
      "Requirement already satisfied: opt-einsum in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jax) (1.22.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jaxlib) (2.0)\n",
      "Requirement already satisfied: six in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from absl-py->jax) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"http://google.com\")       \n",
    "print(r.status_code)\n",
    "\n",
    "# 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**un comment the pretrained models that is medium and small if you want to run text generation on a smaller version of text files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 23:23:51.280583: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLaye  multiple                 774030080 \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 774,030,080\n",
      "Trainable params: 774,030,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#get transformers\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#get large GPT2 tokenizer and GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id =tokenizer.eos_token_id)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#view model parameters\n",
    "GPT2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Different Decoding Methods\n",
    "\n",
    "## First Pass (Greedy Search)   #please refer to geeks for geeks for better understanding of depth first search and backtracking to the node with the most optimised cost function, it doesnt even matter but why not go for it\n",
    "\n",
    "**With Greedy search, the word with the highest probability is predicted as the next word i.e. the next word is updated via:**\n",
    "\n",
    "$$w_t = argmax_{w}P(w | w_{1:t-1})$$\n",
    "\n",
    "**at each timestep $t$. Let's see how this naive approach performs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get deep learning basics\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And there we go boys!! generating text is that easy. Our results are not great - as we can see, our model starts repeating itself rather quickly. Which is kinda weird- bernard looks like having a fit. The main issue with Greedy Search is that words with high probabilities can be masked by words in front of them with low probabilities, so the model is unable to explore more difff combinations of words. We can prevent this by implementing Beam Search which compares difff alternative paths and penalises the sequences:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search with N-Gram Penalities\n",
    "\n",
    "**Beam search is essentially Greedy Search but the model tracks and keeps `num_beams` of hypotheses at each time step, so the model is able to compare alternative paths as it generates text. We can also include a n-gram penalty by setting `no_repeat_ngram_size = 2` which ensures that no 2-grams appear twice. We will also set `num_return_sequences = 5` so we can see what the other 5 beams looked like**\n",
    "\n",
    "**To use Beam Search, we need only modify some parameters in the `generate` function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = GPT2.generate(\n",
    "    input_ids, \n",
    "    max_length = MAX_LEN, \n",
    "    num_beams = 5, \n",
    "    no_repeat_ngram_size = 2, \n",
    "    num_return_sequences = 5, \n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "print('')\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "# now we have 3 output sequences\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**it makes much more sensess if we sampling for reducing K-FOLDS!! LETS  GOOOOOO**\n",
    "\n",
    "**Now that's much better! The 5 different beam statements or predictions or hypothesis are pretty much all the same, but if we increaed `num_beams`, then we would see some more variation in the separate beams. But of course, Beam Search is not perfect either. It works well when the legnth of the generated text is more or less constant, like problems in translation or summarization, but not so much for open-ended problems like dialog or story generation (because it is much harder to find a balance between `num_beams` and `no_repeat_ngram_size`)**\n",
    "\n",
    "**Furthermore, [research](https://arxiv.org/abs/1904.09751) shows that human languages do not follow this 'high probability word next' distribution. This makes sense - if my words were exactly what you expected them to be, I would be quite a boring person and most people don't want to be boring! The below graph plots the difference of Beam Search and actual human speech: ![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)**\n",
    "\n",
    "Taken from original paper [here](https://arxiv.org/abs/1904.09751)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Sampling\n",
    "\n",
    "**Now we will explore indeterministic decodings - sampling. Instead of following a strict path to find the end text with the highest probability, we instead randomly pick the next word by its conditional probability distribution:**\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "**However, when we include this randomness, the generated text tends to be incoherent (see more [here](https://arxiv.org/pdf/1904.09751.pdf)) so we can include the `temperature` parameter which increases the chances of high probability words and decreases the chances of low probability words in the sampling:**\n",
    "\n",
    "**We just need to set `do_sample = True` to implement sampling and for demonstration purposes (you'll shortly see why) we set `top_k = 0`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 0, \n",
    "                             temperature = 0.8\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K Sampling\n",
    "\n",
    "**In Top-K sampling, the top k most likely next words are selected and the entire probability mass is shifted to these k words. So instead of increasing the chances of high probability words occuring and decreasing the chances of low probabillity words, we just remove low probability words all together**\n",
    "\n",
    "**We just need to set `top_k` to however many of the top words we want to consider for our conditional probability distribution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample from only top_k most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top-K Sampling seems to generate more sensical text than our random sampling before. But we can do even better:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P Sampling\n",
    "\n",
    "**Top-P sampling (also known as nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely wordsm we choose the smallest set of words whose total probability is larger than p, and then the entire probability mass is shifted to the words in this set**\n",
    "\n",
    "**The main difference here is that with Top-K sampling, the size of the set of words is static (obviously) whereas in Top-P sampling, the size of the set can change. To use this sampling method, we just set `top_k = 0` and choose a value `top_p`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample only from 80% most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_p = 0.8, \n",
    "                             top_k = 0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NOW LETS DO SOMETHING MORE ABSURD. Lets use dynamic selection size from both top-k and top-p samplings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K and Top-P Sampling\n",
    "\n",
    "**As you could have probably guessed, we can use both Top-K and Top-P sampling here. This reduces the chances of us getting weird words (low probability words) while allowing for a dynamic selection size. We need only top a value for both `top_k` and `top_p`. We can even include the inital temperature parameter if we want to, Let's now see how our model performs now after adding everything together. We will check the top 5 return to see how diverse our answers are:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine both sampling techniques\n",
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .7,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85, \n",
    "                              num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Benchmark Prompts\n",
    "\n",
    "**Here, we will see how well the GPT-2 model does when given some more interesting inputs. The following prompts are taken from [OpenAI's GPT2](https://openai.com/blog/better-language-models/) website where they feed them to their full sized GPT2 model (and the results are astounding, I highly recommend you check out their [page](https://openai.com/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'In a shocking finding, scientist discovered a herd of monsters with three gentials living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the monsters spoke perfect English.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt1, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WELL, ill use bernard to do my friend's desertation project on Cross-cultural management helps understanding the intricasies of business**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"Cross-cultural management helps understanding the intricasies of business\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt2, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically what we can do is deploy a streamlit web pp and deploy my bernard jupyter notebook there. make sure to add component s and widgets to make it look aesthethic. We can use the underlying concept of such models. Please make sure to learn alll components and learn streamlit video tutorials. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st #for web dev\n",
    "\n",
    "\n",
    "st.title(\"Bernard- The first man from earth\")\n",
    "\n",
    "# instantiate the model / download\n",
    "ai = GPT2.generate()\n",
    "\n",
    "# create a prompt text for the text generation \n",
    "#prompt_text = \"Python is awesome\"\n",
    "prompt_text = st.text_input(label = \"What's good ?...\",\n",
    "            value = input_sequence )\n",
    "\n",
    "with st.spinner(\"Bernard is a sentient, with intelligence of an 10000 year old human\"):\n",
    "    # text generation\n",
    "    gpt_text = ai.generate_one(prompt=prompt_text,\n",
    "            max_length = 100 ) \n",
    "st.success(\"AI Successfully generated the below text \")\n",
    "st.balloons()\n",
    "# print ai generated text\n",
    "print(gpt_text)\n",
    "\n",
    "st.text(gpt_text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
