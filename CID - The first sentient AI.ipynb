{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (4.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.22.0)\n",
      "Requirement already satisfied: filelock in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: sacremoses in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: requests in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with HuggingFace - Text Generation\n",
    "\n",
    "\n",
    "\n",
    "**In this notebook, we will explore different decoding methods like Beam search, Top-K sampling, and Top-P sampling, demonstrating their performance along the way. This project is a work in progress and I will rigourslly update it as I learn more about text generation. Feel free to comment with any questions/suggestions:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproducability\n",
    "SEED = 70\n",
    "\n",
    "#maximum number of words in output text\n",
    "MAX_LEN = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Intro\n",
    "\n",
    "** GPT-2 is capable of next word prediction on a much larger and more gigiatuas scale.**\n",
    "\n",
    "**Transformers makes it very easy to import this model with both PyTorch and TensorFlow - in this notebook we will be using TensorFlow but it is just as easy in PyTorch. Both the model and its Tokenizer can be imported from the `transformers` library that anyone can get by typing `!pip install transformers`. We begin with our input seqquence:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = \"after such a tiring day at work, i come back and \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (22.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.1.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1\n",
      "    Uninstalling pip-22.1:\n",
      "      Successfully uninstalled pip-22.1\n",
      "Successfully installed pip-22.1.1\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jax in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (0.3.13)\n",
      "Requirement already satisfied: jaxlib in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (0.3.10)\n",
      "Requirement already satisfied: absl-py in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jax) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jax) (1.22.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jax) (1.6.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jax) (3.7.4.3)\n",
      "Requirement already satisfied: opt-einsum in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from jaxlib) (2.0)\n",
      "Requirement already satisfied: six in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from absl-py->jax) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"http://google.com\")       \n",
    "print(r.status_code)\n",
    "\n",
    "# 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**un comment the pretrained models that is medium and small if you want to run text generation on a smaller version of text files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 06:44:02.430507: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLaye  multiple                 774030080 \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 774,030,080\n",
      "Trainable params: 774,030,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#get transformers\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#get large GPT2 tokenizer and GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id =tokenizer.eos_token_id)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#view model parameters\n",
    "GPT2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Different Decoding Methods\n",
    "\n",
    "## First Pass (Greedy Search)   #please refer to geeks for geeks for better understanding of depth first search and backtracking to the node with the most optimised cost function, it doesnt even matter but why not go for it\n",
    "\n",
    "**With Greedy search, the word with the highest probability is predicted as the next word i.e. the next word is updated via:**\n",
    "\n",
    "$$w_t = argmax_{w}P(w | w_{1:t-1})$$\n",
    "\n",
    "**at each timestep $t$. Let's see how this naive approach performs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get deep learning basics\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "after such a tiring day at work, i come back and ive been working on my new project. i have been working on a new game called \"The Last of Us\" and i have been working on it for about a year now. i have been working on this game for about a year now. i have been working on this game for about a year now. i have been working on this game for about a year now. i have been working on this game for about a year now. i have been working on this\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And there we go boys!! generating text is that easy. Our results are not great - as we can see, our model starts repeating itself rather quickly. Which is kinda weird- bernard looks like having a fit. The main issue with Greedy Search is that words with high probabilities can be masked by words in front of them with low probabilities, so the model is unable to explore more difff combinations of words. We can prevent this by implementing Beam Search which compares difff alternative paths and penalises the sequences:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search with N-Gram Penalities\n",
    "\n",
    "**Beam search is essentially Greedy Search but the model tracks and keeps `num_beams` of hypotheses at each time step, so the model is able to compare alternative paths as it generates text. We can also include a n-gram penalty by setting `no_repeat_ngram_size = 2` which ensures that no 2-grams appear twice. We will also set `num_return_sequences = 5` so we can see what the other 5 beams looked like**\n",
    "\n",
    "**To use Beam Search, we need only modify some parameters in the `generate` function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: after such a tiring day at work, i come back and ive never been so happy in my life.\n",
      "\n",
      "This is the best gift i have ever received. Thank you so much!\n",
      "1: after such a tiring day at work, i come back and ive never been so happy in my life.\n",
      "\n",
      "This is the best gift i have ever received. Thank you so much.\n",
      "2: after such a tiring day at work, i come back and ive never been so happy in my life.\n",
      "\n",
      "This is the best gift i have ever received. Thank you so much. I love it.\n",
      "3: after such a tiring day at work, i come back and ive never been so happy in my life.\n",
      "\n",
      "This is the best gift i have ever received. Thank you so very much.\n",
      "4: after such a tiring day at work, i come back and ive never been so happy in my life.\n",
      "\n",
      "This is the best gift i have ever received. Thank you so much. I love it!\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = GPT2.generate(\n",
    "    input_ids, \n",
    "    max_length = MAX_LEN, \n",
    "    num_beams = 5, \n",
    "    no_repeat_ngram_size = 2, \n",
    "    num_return_sequences = 5, \n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "print('')\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "# now we have 3 output sequences\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**it makes much more sensess if we sampling for reducing K-FOLDS!! LETS  GOOOOOO**\n",
    "\n",
    "**Now that's much better! The 5 different beam statements or predictions or hypothesis are pretty much all the same, but if we increaed `num_beams`, then we would see some more variation in the separate beams. But of course, Beam Search is not perfect either. It works well when the legnth of the generated text is more or less constant, like problems in translation or summarization, but not so much for open-ended problems like dialog or story generation (because it is much harder to find a balance between `num_beams` and `no_repeat_ngram_size`)**\n",
    "\n",
    "**Furthermore, [research](https://arxiv.org/abs/1904.09751) shows that human languages do not follow this 'high probability word next' distribution. This makes sense - if my words were exactly what you expected them to be, I would be quite a boring person and most people don't want to be boring! The below graph plots the difference of Beam Search and actual human speech: ![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)**\n",
    "\n",
    "Taken from original paper [here](https://arxiv.org/abs/1904.09751)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Sampling\n",
    "\n",
    "**Now we will explore indeterministic decodings - sampling. Instead of following a strict path to find the end text with the highest probability, we instead randomly pick the next word by its conditional probability distribution:**\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "**However, when we include this randomness, the generated text tends to be incoherent (see more [here](https://arxiv.org/pdf/1904.09751.pdf)) so we can include the `temperature` parameter which increases the chances of high probability words and decreases the chances of low probability words in the sampling:**\n",
    "\n",
    "**We just need to set `do_sample = True` to implement sampling and for demonstration purposes (you'll shortly see why) we set `top_k = 0`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "after such a tiring day at work, i come back and ive got a sand tan and no colors from my car. i run to the car wash and get some leopard skin leather and paint it over the whole car for my friends to see. so now i have a fake tan and a fake paint job on my car. then i get the black and red sunbed and put it over the fake tan and the fake paint job on my car. then i install the sunbed over the fake tan and the fake paint\n"
     ]
    }
   ],
   "source": [
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 0, \n",
    "                             temperature = 0.8\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K Sampling\n",
    "\n",
    "**In Top-K sampling, the top k most likely next words are selected and the entire probability mass is shifted to these k words. So instead of increasing the chances of high probability words occuring and decreasing the chances of low probabillity words, we just remove low probability words all together**\n",
    "\n",
    "**We just need to set `top_k` to however many of the top words we want to consider for our conditional probability distribution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "after such a tiring day at work, i come back and ive been wondering how much longer i can take the bus by myself from the office, and how many more years will it take before i just give up on the bus, and live by myself for a while. i have had a nice trip to a resort in the northern region of china two months ago, but i was just too tired to move after 4 hours of walking. so i went to a hotel and waited. and then i decided to get on the ...\n"
     ]
    }
   ],
   "source": [
    "#sample from only top_k most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top-K Sampling seems to generate more sensical text than our random sampling before. But we can do even better:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P Sampling\n",
    "\n",
    "**Top-P sampling (also known as nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely wordsm we choose the smallest set of words whose total probability is larger than p, and then the entire probability mass is shifted to the words in this set**\n",
    "\n",
    "**The main difference here is that with Top-K sampling, the size of the set of words is static (obviously) whereas in Top-P sampling, the size of the set can change. To use this sampling method, we just set `top_k = 0` and choose a value `top_p`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "after such a tiring day at work, i come back and ive got to tell you all about it. i don't know how it will help you but i promise you it will. i found the best sex toy on the market.\n",
      "\n",
      "he masturbates almost every day.\n",
      "\n",
      "halloween\n",
      "\n",
      "I have a new friend for Halloween! We talk everyday and he still doesn't know what to wear. When he wore his costume we went shopping and we found this fake penis from the grocery store. i gave it ...\n"
     ]
    }
   ],
   "source": [
    "#sample only from 80% most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_p = 0.8, \n",
    "                             top_k = 0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NOW LETS DO SOMETHING MORE ABSURD. Lets use dynamic selection size from both top-k and top-p samplings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K and Top-P Sampling\n",
    "\n",
    "**As you could have probably guessed, we can use both Top-K and Top-P sampling here. This reduces the chances of us getting weird words (low probability words) while allowing for a dynamic selection size. We need only top a value for both `top_k` and `top_p`. We can even include the inital temperature parameter if we want to, Let's now see how our model performs now after adding everything together. We will check the top 5 return to see how diverse our answers are:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: after such a tiring day at work, i come back and ive just got to make a point and say the word 'bitch'. I really have no idea what to say or do so i just stand there and stare into space. I can feel a wave of disgust roll over me. And then the light goes on.\n",
      "\n",
      "I look up and see that my co-worker has come over and is in tears. \"I thought you were going to say something nice about my mother,\" she says. \"It was a horrible day.\" I say \"I'm sorry.\" I'm so happy she's okay. I'm glad she's on her feet again. I look at the coworker and say \"I'm sorry too\" and walk away.\n",
      "\n",
      "When I'm with my colleagues, I'm often reminded that it's okay to be mad. It's good to be angry at those who hurt us. Anger is healthy. It's good to be angry, it's healthy to be angry.\n",
      "\n",
      "But anger isn't the only emotion to...\n",
      "\n",
      "1: after such a tiring day at work, i come back and ive never felt so much better than in that moment.\n",
      "\n",
      "\n",
      "And i've never felt so healthy in my entire life.\n",
      "\n",
      "\n",
      "I think about this daily. My stomach is full. My blood pressure is just fine.\n",
      "\n",
      "\n",
      "I'm at a loss to describe the feeling of the day. It's not something I can ever describe in words.\n",
      "\n",
      "\n",
      "But i'll try: I've never been so happy in my entire life.\n",
      "\n",
      "\n",
      "I'm not saying that all people get this same experience every day, and everyone does it every single day.\n",
      "\n",
      "\n",
      "But i'm telling you:\n",
      "\n",
      "\n",
      "My body feels like it's supposed to.\n",
      "\n",
      "\n",
      "So, if you're a woman:\n",
      "\n",
      "\n",
      "Take a moment and let yourself relax.\n",
      "\n",
      "\n",
      "Take a moment to breathe.\n",
      "\n",
      "\n",
      "Take a moment to notice what's happening to your body.\n",
      "\n",
      "\n",
      "Notice what it feels like to be relaxed.\n",
      "\n",
      "\n",
      "Let your mind be in peace, let your body rest, and let...\n",
      "\n",
      "2: after such a tiring day at work, i come back and  \"I can't breathe\" was a good excuse to end the day early. \n",
      "The last day of the convention was a bit of a bummer. It was a bit like a vacation for me. I got home to a bunch of boxes from the mail. I took a look at some of the packages that were sitting on my porch. I opened one up, and there was some chocolate chip cookies, and another package of bacon. They seemed pretty decent. I didn't actually eat anything of the bacon that I got, but I have had some bacon in my house before and had a good time with it.\n",
      "I got a package from my friend's fiance. This was the coolest thing ever. The package was made up of a bunch of cute little dinosaur figurines. The first one was a little dinosaur, but he was super cute. He looked like he was about the size of a small dog, and he was covered in feathers and some kind of hide. The next one...\n",
      "\n",
      "3: after such a tiring day at work, i come back and ive been to work, but when i go home i just cant relax. i start to get sleepy, then i get up, i feel like a zombie, i just cant stop thinking about it. i can hardly focus at all. i get tired, i just have to keep going because i have to get the job done and then i go to sleep. i never really wanted to leave but i have had a pretty tough week and it just seems like it wasnt for the best.\n",
      "\n",
      "The problem is that i am very much on the side of being a zombie. i just dont know what to do about it. i really dont like to think of it as a disease or anything like that but i just dont like it. i have been taking some pills (a few pills per day) and i am doing my best. i am in a much better mood and i feel much better when i take them than when i do not take them. I also have been having this thought of going...\n",
      "\n",
      "4: after such a tiring day at work, i come back and ive been able to relax and get some rest. So this time I tried a new tester, and this time i didnt have to wait a day to test again. After this time, my mood was perfect. I was so happy to be able to play my guitar again.\n",
      "\n",
      "\n",
      "I hope this helps,\n",
      "\n",
      "\n",
      "I am really happy to find that you were able to do this with the free trial version of your app, I had no idea you could get this so easy for me.\n",
      "\n",
      "\n",
      "Thank you so much for this, I am so happy to finally be able to do this with the free trial version.\n",
      "\n",
      "\n",
      "I hope you do this for all your users too,\n",
      "\n",
      "\n",
      "Hope this helps,\n",
      "\n",
      "\n",
      "C.\n",
      "\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "\n",
      "Anastasia\n",
      "\n",
      "\n",
      "PS: Sincerely,AnastasiaPS:\n",
      "\n",
      "\n",
      "I think you are an inspiration to me. Your app is just the perfect balance of free and paid, I think I'll give it a...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#combine both sampling techniques\n",
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .7,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85, \n",
    "                              num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Benchmark Prompts\n",
    "\n",
    "**Here, we will see how well model does when given some more interesting inputs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'In a shocking finding, scientist discovered a herd of monsters with three gentials living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the monsters spoke perfect English.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt1, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: In a shocking finding, scientist discovered a herd of monsters with three gentials living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the monsters spoke perfect English.\n",
      "\n",
      "The study was published in the journal Current Biology.\n",
      "\n",
      "\"When they spoke, they were perfectly articulate,\" said Professor Paul Eberhard, a co-author of the study from the School of Earth Sciences at the University of Colorado at Boulder. \"They were saying the things that we say when we're thinking of people.\"\n",
      "\n",
      "The researchers, who are calling the creatures \"bizarre and unusual,\" said the monsters lived in a region of South America known as the Santa Rosa Pampa,...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WELL, ill use bernard to do my friend's desertation project on Cross-cultural management helps understanding the intricasies of business**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"Cross-cultural management helps understanding the intricasies of business\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt2, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Cross-cultural management helps understanding the intricasies of business, but it doesn't do it alone. It doesn't tell you how to solve problems, or how to build new ones. The same is true for the concept of intercultural communication. It's not easy to learn to communicate in different cultures and cultures, and that's just the way it is.\n",
      "\n",
      "One way I've come to understand that is by trying to be more comfortable with my Spanish and having conversations with people in my culture, particularly with people who are also bilingual. For me, the Spanish is like the Italian to English: I can't speak either of them fluently, but I can understand both.\n",
      "\n",
      "As a result, I find it very easy...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
